{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFを要約"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zenn.dev/umi_mori/books/prompt-engineer/viewer/langchain_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install langchain\n",
    "%pip install openai\n",
    "%pip install chromadb\n",
    "%pip install tiktoken\n",
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# secretenvファイルから環境変数をロード\n",
    "secret_path = os.path.expanduser('~/dotsecret')\n",
    "load_dotenv(dotenv_path=secret_path)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChainの基本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "llm.predict(\"hi!\")\n",
    "\n",
    "chat_model.predict(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A good company name for a company that makes colorful socks in Japanese could be \"Iroiro Socks\" (いろいろソックス). \"Iroiro\" means \"various\" or \"many colors\" in Japanese, which perfectly represents the colorful nature of the socks.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks? in japanese\"\n",
    "\n",
    "#llm.predict(text)\n",
    "# >> Feetful of Fun\n",
    "\n",
    "chat_model.predict(text)\n",
    "# >> Socks O'Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to Japanese.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"Japanese\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green', 'yellow', 'purple']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser()\n",
    ")\n",
    "chain.run(\"colors\")\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='コンピュータ技術を活用してシステム開発やネットワーク構築を行う技術者。', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "chat([\n",
    "\tSystemMessage(content=\"日本語で回答して。\"),\n",
    "\tHumanMessage(content=\"ITエンジニアについて30文字で教えて。\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストを埋め込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n",
      "\n",
      "\n",
      "Q1. インターネット上の何のデータを使って、学習しているの？\n",
      " GPT-4は、インターネット上の大量のテキストデータを学習しています。\n",
      "{'question': 'Q1. インターネット上の何のデータを使って、学習しているの？', 'answer': ' GPT-4 is trained using a large amount of text data from the internet.\\n', 'sources': './long_text.txt'}\n",
      "\n",
      "\n",
      "Q2. GPT4は第何世代のモデル？\n",
      " GPT-4は、OpenAIが開発したAI技術であるGPTシリーズの第4世代目のモデルです。\n",
      "{'question': 'Q2. GPT4は第何世代のモデル？', 'answer': ' GPT-4 is the fourth generation model of the GPT series developed by OpenAI.\\n', 'sources': './long_text.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "long_text = \"\"\"\n",
    "GPT-4は、OpenAIが開発したAI技術であるGPTシリーズの第4世代目のモデルです。\n",
    "\n",
    "自然言語処理(NLP)という技術を使い、文章の生成や理解を行うことができます。\n",
    "\n",
    "これにより、人間と同じような文章を作成することが可能です。\n",
    "\n",
    "GPT-4は、トランスフォーマーアーキテクチャに基づいており、より強力な性能を発揮します。\n",
    "\n",
    "GPT-4は、インターネット上の大量のテキストデータを学習し、豊富な知識を持っています。\n",
    "\n",
    "しかし、2021年9月までの情報しか持っていません。\n",
    "\n",
    "このモデルは、質問応答や文章生成、文章要約など、様々なタスクで使用できます。\n",
    "\n",
    "ただし、GPT-4は完璧ではありません。\n",
    "\n",
    "時々、誤った情報や不適切な内容を生成することがあります。\n",
    "\n",
    "使用者は、その限界を理解し、\n",
    "\n",
    "適切な方法で利用することが重要です。\n",
    "\"\"\"\n",
    "print(len(long_text))\n",
    "with open(\"./datasource/long_text.txt\", \"w\") as f:\n",
    "    f.write(long_text)\n",
    "    f.close()\n",
    "\n",
    "loader = TextLoader('./datasource/long_text.txt')\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Chroma, # Default\n",
    "    embedding=OpenAIEmbeddings(), # Default\n",
    "    text_splitter=text_splitter,\n",
    ").from_loaders([loader])\n",
    "\n",
    "\n",
    "\n",
    "query = \"Q1. インターネット上の何のデータを使って、学習しているの？\"\n",
    "print(f\"\\n\\n{query}\")\n",
    "answer = index.query(query)\n",
    "print(answer)\n",
    "\n",
    "answer_with_sources = index.query_with_sources(query)\n",
    "print(answer_with_sources)\n",
    "\n",
    "query = \"Q2. GPT4は第何世代のモデル？\"\n",
    "print(f\"\\n\\n{query}\")\n",
    "answer = index.query(query)\n",
    "print(answer)\n",
    "\n",
    "answer_with_sources = index.query_with_sources(query)\n",
    "print(answer_with_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFを学習させたChatGPTの実装方法\n",
    "\n",
    "[PDFを学習させたChatGPTの実装方法【Python / LangChain / FAQチャットボット】](https://zenn.dev/umi_mori/articles/langchain-indexes-pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 878名(内フリーランス・パラレルキャリア活動者 850名)\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = PyPDFLoader(\"https://blog.freelance-jp.org/wp-content/uploads/2023/03/FreelanceSurvey2023.pdf\")\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Chroma, # Default\n",
    "    embedding=OpenAIEmbeddings(), # Default\n",
    ").from_loaders([loader])\n",
    "\n",
    "query = \"フリーランス白書のPDFにおける、今回の調査の有効回答数は？\"\n",
    "\n",
    "answer = index.query(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFからindexを生成してディレクトリに永続化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " リライアビリティが最も重要な機能であることを理解し、それを実現するために、ソフトウェアと人間が協力して協調しなければならないことを理解することです。\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# データの追加\n",
    "loader_sre = PyPDFLoader(os.path.expanduser('./the-site-reliability-workbook-next18.pdf'))\n",
    "\n",
    "index_sre = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=Chroma,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    vectorstore_kwargs={\"persist_directory\": \"datastore\"}, # datastoreフォルダにvectorstoreを保存する\n",
    ").from_loaders([loader_sre])\n",
    "\n",
    "# 検索\n",
    "query_sre = \"site-reliabilityのために一番最初にするべきことは？日本語で答えてください\"\n",
    "answer_sre = index_sre.query(query_sre)\n",
    "print(answer_sre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' インタラプトを受ける時間を予定することを避けるべきです。ケーススタディ2では、リモートチームがオンコールを減らしたとき、チームメンバーはプロジェクトに集中する時間を得ることができました。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_sre.query(\"避けるべきことはなんですか？日本語でお願いします\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 一般社団法人プロフェッショナル＆パラレルキャリアフリーランス協会'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"PDFの著者は？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChainでindex化されたPDFをデータストアから読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " リライアビリティが最も重要な機能であることを理解し、それを実現するために、ソフトウェアと人間が協力して協調しなければならないことを理解することです。\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"datastore\", embedding_function=OpenAIEmbeddings())\n",
    "index_from_store = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "\n",
    "# 検索\n",
    "query = \"site-reliabilityのために一番最初にするべきことは？日本語で答えてください\"\n",
    "answer = index_from_store.query(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "vectorstore = Chroma(persist_directory=\"datastore\", embedding_function=OpenAIEmbeddings())\n",
    "index_from_store = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "\n",
    "# 検索\n",
    "query = \"site-reliabilityのために一番最初にするべきことは？日本語で答えてください\"\n",
    "answer = index_from_store.query(query, llm=ChatOpenAI(model_name=\"gpt-4\"))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 第1冊のSRE本と合わせて読むことをお勧めします。本書では、GoogleのSREが実践している方法を中心に、The Home DepotやThe New York Timesなどの伝統的な企業からデジタルスタートアップまで、他の企業からの視点も含めて扱っています。'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_from_store.query(\"本書の要点を教えてください\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
